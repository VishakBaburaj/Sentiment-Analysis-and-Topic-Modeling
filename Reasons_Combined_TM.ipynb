{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vishak Baburaj\n",
    "# Topic Modelling\n",
    "# Combining both the responses of students \n",
    "# Reasons why the students want the exams to be cancelled\n",
    "# Reasons why the students want the exams to be conducted on a later date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\visha\\Desktop\\Topic Modeling Response Combined.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"combined_response\"])\n",
    "data = data.iloc[1:]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['combined_response']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = data['combined_response'].str.lower()\n",
    "data['combined_response_preprocess'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess']=data['combined_response_preprocess'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(sentence,punctuation=False): \n",
    "    sentence = sentence.strip() \n",
    "    if punctuation: \n",
    "        PATTERN = string.punctuation\n",
    "        filtered_sentence = re.sub(PATTERN, r' ',sentence) \n",
    "    else: \n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'  \n",
    "        filtered_sentence = re.sub(PATTERN, r' ',sentence) \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = [remove_special_characters(sentence) for sentence in data['combined_response_preprocess']] \n",
    "data['combined_response_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Multiple Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = data['combined_response_preprocess'].replace('\\s+', ' ', regex = True)\n",
    "data['combined_response_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = data['combined_response_preprocess'].str.replace('\\d+','')\n",
    "data['combined_response_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Spelling Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = data['combined_response_preprocess'].apply(lambda x: \" \".join([spell(i) for i in x.split()]))\n",
    "data['combined_response_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatizing\n",
    "data['combined_response_preprocess'] = data['combined_response_preprocess'].apply(lambda x: lemmatize_sentence(x))\n",
    "data['combined_response_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "#STOPWORDS.extend(['exam','covid','conduct','cancel','pandemic','corona','get','due','student','board','also','write','us','mark','go','increase','many','want','take','important','first','reason','well','etc','able','year','need','purpose','must','unable','class','please','good','even','day','th','one','exams','would','know','give','much','may','make'])\n",
    "#STOPWORDS.extend(['exam','cancel','conduct','student','board','get','also','pandemic','want','us','first','year','mark','go','take','many','th','one','even','need','covid','due','important','corona','reason','increase','day','able','well','etc','case','must','anything','class','write','please','exams','give','would','understand','know','much'])\n",
    "STOPWORDS.extend(['covid','important','due','get','also','us','many','go','take','please','able','well','exam','cancel','board','want','exams','can','would','much','corona','student','conduct','mark','need','pandemic','corona','would','exam','because','next','etc','pu','st','th'])\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['combined_response_preprocess'] = data['combined_response_preprocess'].apply(lambda text: remove_stopwords(text))\n",
    "print(data['combined_response_preprocess'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(data['combined_response_preprocess'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=2000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "data1 = data['combined_response_preprocess'].values.tolist()\n",
    "data_words = list(sent_to_words(data1))\n",
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_words))\n",
    "print(type(data_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "print(len(id2word))\n",
    "id2word.filter_extremes(no_below=15, no_above=0.5)\n",
    "print(len(id2word))\n",
    "#id2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from pprint import pprint\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA MODEL TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "chunksize = 38237\n",
    "passes = 50\n",
    "iterations = 1000\n",
    "eval_every = None\n",
    "lda_model_tfidf = gensim.models.ldamodel.LdaModel(corpus_tfidf,\n",
    "                                            id2word=id2word,\n",
    "                                            chunksize=chunksize,\n",
    "                                            alpha='auto',\n",
    "                                            eta='auto',\n",
    "                                            iterations=iterations,\n",
    "                                            num_topics=num_topics,\n",
    "                                            passes=passes,\n",
    "                                            eval_every=eval_every,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "pprint(lda_model_tfidf.print_topics())\n",
    "doc_lda_tfidf = lda_model_tfidf[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity AND Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_tfidf.log_perplexity(corpus_tfidf)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda_tfidf = CoherenceModel(model=lda_model_tfidf, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_tfidf = coherence_model_lda_tfidf.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_tfidf = gensimvis.prepare(lda_model_tfidf, corpus_tfidf, dictionary=lda_model_tfidf.id2word)\n",
    "vis_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics_tfidf = lda_model_tfidf.top_topics(corpus_tfidf) #, num_words=20)\n",
    "top_topics_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
